{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoHDR Kaggle Training Notebook\n",
    "\n",
    "Trains an edge-aware MicroUNet model for automatic lens correction.\n",
    "\n",
    "If `/kaggle/input` has no mounted data, this notebook falls back to downloading the competition zip into `/kaggle/working`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import zipfile\n",
    "import argparse\n",
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Optional token override; leave empty unless explicitly injected via notebook environment.\n",
    "FALLBACK_KAGGLE_TOKEN = os.getenv(\"KAGGLE_API_TOKEN\", \"\").strip()\n",
    "\n",
    "\n",
    "def candidate_roots(root_hint: str) -> List[str]:\n",
    "    roots: List[str] = []\n",
    "\n",
    "    def add(p: str):\n",
    "        if p and os.path.isdir(p) and p not in roots:\n",
    "            roots.append(p)\n",
    "\n",
    "    add(root_hint)\n",
    "    add('/kaggle/input')\n",
    "\n",
    "    if os.path.isdir('/kaggle/input'):\n",
    "        for name in sorted(os.listdir('/kaggle/input')):\n",
    "            add(os.path.join('/kaggle/input', name))\n",
    "\n",
    "    return roots\n",
    "\n",
    "\n",
    "def extract_zip_if_needed(root: str) -> str:\n",
    "    zip_files = []\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for fname in filenames:\n",
    "            if fname.lower().endswith('.zip'):\n",
    "                zip_files.append(os.path.join(dirpath, fname))\n",
    "\n",
    "    if not zip_files:\n",
    "        return root\n",
    "\n",
    "    extract_root = '/kaggle/working/autohdr_input_cache'\n",
    "    os.makedirs(extract_root, exist_ok=True)\n",
    "\n",
    "    extracted_any = False\n",
    "    for zpath in zip_files[:8]:\n",
    "        target = os.path.join(extract_root, os.path.splitext(os.path.basename(zpath))[0])\n",
    "        if os.path.isdir(target) and any(os.scandir(target)):\n",
    "            continue\n",
    "        print(f'[auto-detect] Extracting {zpath} -> {target}')\n",
    "        os.makedirs(target, exist_ok=True)\n",
    "        try:\n",
    "            with zipfile.ZipFile(zpath, 'r') as zf:\n",
    "                zf.extractall(target)\n",
    "            extracted_any = True\n",
    "        except Exception as exc:\n",
    "            print(f'[auto-detect] Skipping unreadable zip {zpath}: {exc}')\n",
    "\n",
    "    return extract_root if extracted_any else root\n",
    "\n",
    "\n",
    "def find_dataset_paths(root_hint: str) -> Tuple[str, Optional[str]]:\n",
    "    roots = candidate_roots(root_hint)\n",
    "    print('Candidate roots:')\n",
    "    for r in roots:\n",
    "        print(f'  - {r}')\n",
    "\n",
    "    for base in roots:\n",
    "        train_dir = None\n",
    "        test_dir = None\n",
    "\n",
    "        for root in [base, extract_zip_if_needed(base)]:\n",
    "            for dirpath, _, filenames in os.walk(root):\n",
    "                base_name = os.path.basename(dirpath)\n",
    "                if base_name == 'lens-correction-train-cleaned':\n",
    "                    train_dir = dirpath\n",
    "                elif base_name == 'test-originals':\n",
    "                    test_dir = dirpath\n",
    "                if train_dir and test_dir:\n",
    "                    return train_dir, test_dir\n",
    "\n",
    "            if not train_dir:\n",
    "                for dirpath, _, filenames in os.walk(root):\n",
    "                    originals = [f for f in filenames if f.endswith('_original.jpg')]\n",
    "                    if len(originals) > 100:\n",
    "                        train_dir = dirpath\n",
    "                        print(f'[auto-detect] Found {len(originals)} training originals in {dirpath}')\n",
    "                        break\n",
    "\n",
    "            if not test_dir:\n",
    "                for dirpath, _, filenames in os.walk(root):\n",
    "                    jpgs = [f for f in filenames if f.endswith('.jpg')]\n",
    "                    if 500 < len(jpgs) < 5000:\n",
    "                        has_pairs = any(\n",
    "                            f.endswith('_original.jpg') or f.endswith('_generated.jpg')\n",
    "                            for f in jpgs[:200]\n",
    "                        )\n",
    "                        if not has_pairs:\n",
    "                            test_dir = dirpath\n",
    "                            print(f'[auto-detect] Found {len(jpgs)} test images in {dirpath}')\n",
    "                            break\n",
    "\n",
    "            if train_dir:\n",
    "                return train_dir, test_dir\n",
    "\n",
    "    raise RuntimeError('Could not find training data under scanned roots.')\n",
    "\n",
    "\n",
    "def ensure_kaggle_cli_data(download_root: str, max_pairs: int = 3000) -> str:\n",
    "    os.makedirs(download_root, exist_ok=True)\n",
    "\n",
    "    token = os.getenv('KAGGLE_API_TOKEN', '').strip() or FALLBACK_KAGGLE_TOKEN\n",
    "\n",
    "    # Configure credentials when an explicit token is available.\n",
    "    kaggle_dir = '/root/.kaggle'\n",
    "    os.makedirs(kaggle_dir, exist_ok=True)\n",
    "    if token:\n",
    "        with open(os.path.join(kaggle_dir, 'access_token'), 'w') as f:\n",
    "            f.write(token)\n",
    "        with open(os.path.join(kaggle_dir, 'kaggle.json'), 'w') as f:\n",
    "            f.write(json.dumps({'username': 'token', 'key': token.replace('KGAT_', '')}))\n",
    "        os.chmod(os.path.join(kaggle_dir, 'access_token'), 0o600)\n",
    "        os.chmod(os.path.join(kaggle_dir, 'kaggle.json'), 0o600)\n",
    "\n",
    "    if shutil.which('kaggle') is None:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'kaggle'], check=False)\n",
    "\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    except Exception:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'kaggle'], check=True)\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    api = KaggleApi()\n",
    "    try:\n",
    "        api.authenticate()\n",
    "    except Exception as exc:\n",
    "        message = (\n",
    "            \"Fallback Kaggle API auth failed. \"\n",
    "            \"Attach competition data source or set KAGGLE_API_TOKEN.\\n\"\n",
    "            f\"Auth error: {exc}\"\n",
    "        )\n",
    "        raise RuntimeError(message)\n",
    "\n",
    "    train_rel = 'lens-correction-train-cleaned'\n",
    "    train_dir = os.path.join(download_root, train_rel)\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "    existing_originals = [f for f in os.listdir(train_dir) if f.endswith('_original.jpg')]\n",
    "    if len(existing_originals) >= max_pairs:\n",
    "        print(f'[fallback] Using existing cached training files: {len(existing_originals)} originals')\n",
    "        return download_root\n",
    "\n",
    "    print('[fallback] Listing competition files...')\n",
    "    files_response = api.competition_list_files('automatic-lens-correction')\n",
    "    if hasattr(files_response, 'files'):\n",
    "        files_iter = files_response.files\n",
    "    elif isinstance(files_response, list):\n",
    "        files_iter = files_response\n",
    "    else:\n",
    "        try:\n",
    "            files_iter = list(files_response)\n",
    "        except TypeError as exc:\n",
    "            raise RuntimeError(f'Unexpected list-files response type: {type(files_response)}') from exc\n",
    "\n",
    "    originals = []\n",
    "    for item in files_iter:\n",
    "        name = getattr(item, 'name', None)\n",
    "        if name is None and isinstance(item, dict):\n",
    "            name = item.get('name')\n",
    "        if not name:\n",
    "            continue\n",
    "        if name.startswith(train_rel + '/') and name.endswith('_original.jpg'):\n",
    "            originals.append(name)\n",
    "    originals.sort()\n",
    "\n",
    "    if not originals:\n",
    "        raise RuntimeError('No training originals found in competition file listing.')\n",
    "\n",
    "    selected_originals = originals[: max(1, int(max_pairs))]\n",
    "    required = set(selected_originals)\n",
    "    required.update(name.replace('_original.jpg', '_generated.jpg') for name in selected_originals)\n",
    "\n",
    "    to_download = []\n",
    "    for rel_path in sorted(required):\n",
    "        abs_path = os.path.join(download_root, rel_path)\n",
    "        if not os.path.exists(abs_path):\n",
    "            to_download.append(rel_path)\n",
    "\n",
    "    print(\n",
    "        f'[fallback] Downloading {len(to_download)} files '\n",
    "        f'for {len(selected_originals)} training pairs...'\n",
    "    )\n",
    "\n",
    "    for i, rel_path in enumerate(to_download, start=1):\n",
    "        local_dir = os.path.join(download_root, os.path.dirname(rel_path))\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        file_name = os.path.basename(rel_path)\n",
    "\n",
    "        api.competition_download_file(\n",
    "            competition='automatic-lens-correction',\n",
    "            file_name=rel_path,\n",
    "            path=local_dir,\n",
    "            quiet=True,\n",
    "        )\n",
    "\n",
    "        downloaded_path = os.path.join(local_dir, file_name)\n",
    "        zip_candidate = downloaded_path + '.zip'\n",
    "        if os.path.exists(zip_candidate):\n",
    "            with zipfile.ZipFile(zip_candidate, 'r') as zf:\n",
    "                zf.extractall(local_dir)\n",
    "            os.remove(zip_candidate)\n",
    "\n",
    "        if i % 200 == 0 or i == len(to_download):\n",
    "            print(f'[fallback] Downloaded {i}/{len(to_download)} files')\n",
    "\n",
    "    return download_root\n",
    "\n",
    "\n",
    "class AutoHDRDataset(Dataset):\n",
    "    def __init__(self, samples: List[dict], mode: str, transform=None):\n",
    "        self.samples = samples\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        item = self.samples[idx]\n",
    "        original = Image.open(item['original']).convert('RGB')\n",
    "        if self.transform:\n",
    "            original = self.transform(original)\n",
    "        out = {'original': original}\n",
    "        if self.mode in ('train', 'val'):\n",
    "            generated = Image.open(item['generated']).convert('RGB')\n",
    "            if self.transform:\n",
    "                generated = self.transform(generated)\n",
    "            out['generated'] = generated\n",
    "        else:\n",
    "            out['filename'] = item['filename']\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_sample_lists(train_dir: str, test_dir: Optional[str], max_train=None, max_val=None):\n",
    "    originals = sorted(glob.glob(os.path.join(train_dir, '*_original.jpg')))\n",
    "    pairs = []\n",
    "    for op in originals:\n",
    "        gp = op.replace('_original.jpg', '_generated.jpg')\n",
    "        if os.path.exists(gp):\n",
    "            pairs.append({'original': op, 'generated': gp})\n",
    "\n",
    "    split = int(len(pairs) * 0.95)\n",
    "    train_samples = pairs[:split]\n",
    "    val_samples = pairs[split:]\n",
    "\n",
    "    if max_train:\n",
    "        train_samples = train_samples[:max_train]\n",
    "    if max_val:\n",
    "        val_samples = val_samples[:max_val]\n",
    "\n",
    "    test_samples = []\n",
    "    if test_dir and os.path.isdir(test_dir):\n",
    "        for f in sorted(os.listdir(test_dir)):\n",
    "            if f.endswith('.jpg'):\n",
    "                test_samples.append({'original': os.path.join(test_dir, f), 'filename': f})\n",
    "\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "def make_dataloaders(train_dir: str, test_dir: Optional[str], batch_size=8, img_size=256, num_workers=2, max_train=None, max_val=None):\n",
    "    train_tfm = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.02),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    eval_tfm = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_samples, val_samples, test_samples = build_sample_lists(train_dir, test_dir, max_train=max_train, max_val=max_val)\n",
    "    print(f'Train samples: {len(train_samples):,}')\n",
    "    print(f'Val samples: {len(val_samples):,}')\n",
    "    print(f'Test samples: {len(test_samples):,}')\n",
    "\n",
    "    kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': num_workers,\n",
    "        'pin_memory': torch.cuda.is_available(),\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(AutoHDRDataset(train_samples, 'train', train_tfm), shuffle=True, **kwargs)\n",
    "    val_loader = DataLoader(AutoHDRDataset(val_samples, 'val', eval_tfm), shuffle=False, **kwargs)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MicroUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_channels, 16)\n",
    "        self.enc2 = ConvBlock(16, 32)\n",
    "        self.enc3 = ConvBlock(32, 64)\n",
    "        self.enc4 = ConvBlock(64, 128)\n",
    "        self.bottleneck = ConvBlock(128, 256)\n",
    "        self.up4 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(256, 128, 3, padding=1))\n",
    "        self.dec4 = ConvBlock(256, 128)\n",
    "        self.up3 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(128, 64, 3, padding=1))\n",
    "        self.dec3 = ConvBlock(128, 64)\n",
    "        self.up2 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(64, 32, 3, padding=1))\n",
    "        self.dec2 = ConvBlock(64, 32)\n",
    "        self.up1 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(32, 16, 3, padding=1))\n",
    "        self.dec1 = ConvBlock(32, 16)\n",
    "        self.out_conv = nn.Conv2d(16, out_channels, 1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], 1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], 1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], 1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], 1))\n",
    "        return torch.sigmoid(self.out_conv(d1))\n",
    "\n",
    "\n",
    "class SobelFilter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        gx = torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 0.0, 2.0], [-1.0, 0.0, 1.0]]) / 4.0\n",
    "        gy = torch.tensor([[-1.0, -2.0, -1.0], [0.0, 0.0, 0.0], [1.0, 2.0, 1.0]]) / 4.0\n",
    "        self.gx = gx.view(1, 1, 3, 3)\n",
    "        self.gy = gy.view(1, 1, 3, 3)\n",
    "\n",
    "    def forward(self, img):\n",
    "        gx = self.gx.to(img.device)\n",
    "        gy = self.gy.to(img.device)\n",
    "        gray = 0.2989 * img[:, 0:1] + 0.5870 * img[:, 1:2] + 0.1140 * img[:, 2:3]\n",
    "        grad_x = nn.functional.conv2d(gray, gx, padding=1)\n",
    "        grad_y = nn.functional.conv2d(gray, gy, padding=1)\n",
    "        return torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, l1_weight=1.0, edge_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.sobel = SobelFilter()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.edge_weight = edge_weight\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pixel = self.l1(pred, target)\n",
    "        edge = self.l1(self.sobel(pred), self.sobel(target))\n",
    "        return self.l1_weight * pixel + self.edge_weight * edge\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        x = batch['original'].to(device, non_blocking=True)\n",
    "        y = batch['generated'].to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total += loss.item()\n",
    "        n += 1\n",
    "    return total / max(n, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        x = batch['original'].to(device, non_blocking=True)\n",
    "        y = batch['generated'].to(device, non_blocking=True)\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        mae_255 = (pred - y).abs().mean().item() * 255.0\n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae_255\n",
    "        n += 1\n",
    "    return total_loss / max(n, 1), total_mae / max(n, 1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='AutoHDR Kaggle MicroUNet training')\n",
    "    parser.add_argument('--data-root', default='/kaggle/input')\n",
    "    parser.add_argument('--output-dir', default='/kaggle/working')\n",
    "    parser.add_argument('--epochs', type=int, default=20)\n",
    "    parser.add_argument('--batch-size', type=int, default=8)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--img-size', type=int, default=256)\n",
    "    parser.add_argument('--num-workers', type=int, default=2)\n",
    "    parser.add_argument('--max-train', type=int, default=None)\n",
    "    parser.add_argument('--max-val', type=int, default=None)\n",
    "    parser.add_argument('--save-every', type=int, default=2)\n",
    "    parser.add_argument('--edge-weight', type=float, default=0.5)\n",
    "    parser.add_argument('--no-amp', action='store_true')\n",
    "    parser.add_argument('--fallback-max-pairs', type=int, default=3000)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    device = get_device()\n",
    "    print(f'Device: {device}')\n",
    "    print(f'PyTorch: {torch.__version__}')\n",
    "    if device.type == 'cuda':\n",
    "        print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "    print('Locating dataset...')\n",
    "    try:\n",
    "        train_dir, test_dir = find_dataset_paths(args.data_root)\n",
    "    except Exception as exc:\n",
    "        print(f'[fallback] Primary discovery failed: {exc}')\n",
    "        dl_root = ensure_kaggle_cli_data(\n",
    "            '/kaggle/working/autohdr_competition_data',\n",
    "            max_pairs=args.fallback_max_pairs,\n",
    "        )\n",
    "        train_dir, test_dir = find_dataset_paths(dl_root)\n",
    "\n",
    "    print(f'Train dir: {train_dir}')\n",
    "    print(f'Test dir: {test_dir}')\n",
    "\n",
    "    train_loader, val_loader = make_dataloaders(\n",
    "        train_dir=train_dir,\n",
    "        test_dir=test_dir,\n",
    "        batch_size=args.batch_size,\n",
    "        img_size=args.img_size,\n",
    "        num_workers=args.num_workers,\n",
    "        max_train=args.max_train,\n",
    "        max_val=args.max_val,\n",
    "    )\n",
    "\n",
    "    model = MicroUNet().to(device)\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'Model: MicroUNet ({params:,} params)')\n",
    "\n",
    "    criterion = CombinedLoss(l1_weight=1.0, edge_weight=args.edge_weight).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "    use_amp = device.type == 'cuda' and not args.no_amp\n",
    "    scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "    if use_amp:\n",
    "        print('Mixed precision: ON')\n",
    "\n",
    "    history = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler=scaler)\n",
    "        val_loss, val_mae = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        improved = val_loss < best_val_loss\n",
    "        marker = ' \u2605' if improved else ''\n",
    "        print(\n",
    "            f'Epoch {epoch:3d}/{args.epochs} | '\n",
    "            f'Train: {train_loss:.5f} | '\n",
    "            f'Val: {val_loss:.5f} | '\n",
    "            f'MAE(255): {val_mae:.2f} | '\n",
    "            f'LR: {lr:.6f} | '\n",
    "            f'{elapsed:.1f}s{marker}'\n",
    "        )\n",
    "\n",
    "        entry = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': round(train_loss, 6),\n",
    "            'val_loss': round(val_loss, 6),\n",
    "            'val_mae_255': round(val_mae, 4),\n",
    "            'lr': round(lr, 10),\n",
    "            'time_s': round(elapsed, 3),\n",
    "            'timestamp_utc': datetime.now(timezone.utc).isoformat(),\n",
    "        }\n",
    "        history.append(entry)\n",
    "\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_mae_255': val_mae,\n",
    "            'model_name': 'micro_unet',\n",
    "            'img_size': args.img_size,\n",
    "            'normalize': False,\n",
    "            'created_at': datetime.now(timezone.utc).isoformat(),\n",
    "            'args': vars(args),\n",
    "        }\n",
    "\n",
    "        if improved:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(ckpt, os.path.join(args.output_dir, 'best_model.pt'))\n",
    "\n",
    "        if epoch % max(args.save_every, 1) == 0:\n",
    "            torch.save(ckpt, os.path.join(args.output_dir, f'checkpoint_epoch{epoch}.pt'))\n",
    "\n",
    "    history_path = os.path.join(args.output_dir, 'training_history.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    print('=' * 70)\n",
    "    print(f'Training complete. Best val loss: {best_val_loss:.6f}')\n",
    "    print(f'Artifacts in: {args.output_dir}')\n",
    "    print('=' * 70)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
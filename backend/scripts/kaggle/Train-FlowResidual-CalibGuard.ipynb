{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoHDR Flow-Residual CalibGuard Notebook\n",
    "\n",
    "Self-contained Kaggle notebook for exploratory dimension-conditioned residual flow correction.\n",
    "\n",
    "Outputs under `/kaggle/working/autohdr_flow_residual/`:\n",
    "- `submission_flow_residual_dim_v1_<timestamp>.zip`\n",
    "- `submission_flow_residual_dim_base_<timestamp>.zip`\n",
    "- `run_summary_<timestamp>.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = int(os.getenv(\"SEED\", \"42\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MIN_SUPPORT = int(os.getenv(\"MIN_SUPPORT\", \"100\"))\n",
    "TRAIN_SAMPLE_PER_DIM = int(os.getenv(\"TRAIN_SAMPLE_PER_DIM\", \"400\"))\n",
    "HOLDOUT_RATIO = float(os.getenv(\"HOLDOUT_RATIO\", \"0.2\"))\n",
    "FLOW_MAX_MAG = float(os.getenv(\"FLOW_MAX_MAG\", \"40.0\"))\n",
    "LAMBDA_GRID = [float(x) for x in os.getenv(\"LAMBDA_GRID\", \"0,0.02,0.04,0.06,0.08,0.10\").split(\",\") if x.strip()]\n",
    "ACCEPT_MIN_PROXY_GAIN = float(os.getenv(\"ACCEPT_MIN_PROXY_GAIN\", \"0.002\"))\n",
    "PROFILE = os.getenv(\"PROFILE\", \"balanced\")\n",
    "JPEG_QUALITY = int(os.getenv(\"JPEG_QUALITY\", \"95\"))\n",
    "\n",
    "FLOW_WORK_SCALE = 0.25\n",
    "FLOW_SMOOTH_SIGMA = 1.2\n",
    "FLOW_STABILITY_VAR_MAX = 3.5\n",
    "BORDER_RATIO_MAX = 0.002\n",
    "WARP_RISK_MAX = 0.35\n",
    "\n",
    "WORK_ROOT = Path(\"/kaggle/working/autohdr_flow_residual\")\n",
    "CANDIDATE_DIR = WORK_ROOT / \"candidate\"\n",
    "BASE_DIR = WORK_ROOT / \"base\"\n",
    "WORK_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "CANDIDATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_DIM_TABLE = {\n",
    "    \"global_fallback\": {\"k1\": -0.17, \"k2\": 0.35},\n",
    "    \"parent_classes\": {\n",
    "        \"standard\": {\"primary\": {\"k1\": -0.17, \"k2\": 0.35}, \"safe\": {\"k1\": -0.165, \"k2\": 0.34}},\n",
    "        \"near_standard_tall\": {\"primary\": {\"k1\": -0.17, \"k2\": 0.35}, \"safe\": {\"k1\": -0.165, \"k2\": 0.34}},\n",
    "        \"near_standard_short\": {\"primary\": {\"k1\": -0.17, \"k2\": 0.35}, \"safe\": {\"k1\": -0.16, \"k2\": 0.33}},\n",
    "        \"moderate_crop\": {\"primary\": {\"k1\": -0.08, \"k2\": 0.15}, \"safe\": {\"k1\": -0.055, \"k2\": 0.1}},\n",
    "        \"heavy_crop\": {\"primary\": {\"k1\": -0.01, \"k2\": 0.01}, \"safe\": {\"k1\": -0.002, \"k2\": 0.005}},\n",
    "        \"portrait_standard\": {\"primary\": {\"k1\": -0.02, \"k2\": 0.02}, \"safe\": {\"k1\": -0.008, \"k2\": 0.015}},\n",
    "        \"portrait_cropped\": {\"primary\": {\"k1\": -0.005, \"k2\": 0.005}, \"safe\": {\"k1\": 0.0, \"k2\": 0.0}},\n",
    "    },\n",
    "    \"dimensions\": {},\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DimModel:\n",
    "    dim_key: str\n",
    "    support: int\n",
    "    parent_class: str\n",
    "    k1: float\n",
    "    k2: float\n",
    "    model_type: str\n",
    "    lambda_value: float\n",
    "    accepted: bool\n",
    "    reason: str\n",
    "    proxy_gain: float\n",
    "    base_loss: float\n",
    "    best_loss: float\n",
    "    border_ratio: float\n",
    "    flow_var: float\n",
    "    flow_p99: float\n",
    "    flow_clip_max: float\n",
    "    warp_risk: float\n",
    "    flow_map: np.ndarray | None\n",
    "\n",
    "\n",
    "def classify_parent(height: int, width: int) -> str:\n",
    "    is_portrait = height > width\n",
    "    short_edge = width if is_portrait else height\n",
    "    if is_portrait:\n",
    "        return \"portrait_standard\" if short_edge == 1367 else \"portrait_cropped\"\n",
    "    if short_edge == 1367:\n",
    "        return \"standard\"\n",
    "    if short_edge in (1368, 1369, 1370, 1371):\n",
    "        return \"near_standard_tall\"\n",
    "    if short_edge in (1365, 1366):\n",
    "        return \"near_standard_short\"\n",
    "    if 1360 <= short_edge <= 1364:\n",
    "        return \"moderate_crop\"\n",
    "    return \"heavy_crop\"\n",
    "\n",
    "\n",
    "def choose_coeffs(table: dict, dim_key: str, parent_class: str, profile: str):\n",
    "    dimensions = table.get(\"dimensions\", {})\n",
    "    parent_classes = table.get(\"parent_classes\", {})\n",
    "    global_fallback = table.get(\"global_fallback\", {\"k1\": -0.17, \"k2\": 0.35})\n",
    "    entry = dimensions.get(dim_key)\n",
    "    parent = parent_classes.get(parent_class, {})\n",
    "\n",
    "    def fallback_primary():\n",
    "        node = parent.get(\"primary\")\n",
    "        if isinstance(node, dict) and \"k1\" in node and \"k2\" in node:\n",
    "            return float(node[\"k1\"]), float(node[\"k2\"]), \"parent_primary\"\n",
    "        return float(global_fallback[\"k1\"]), float(global_fallback[\"k2\"]), \"global_fallback\"\n",
    "\n",
    "    def fallback_safe():\n",
    "        node = parent.get(\"safe\")\n",
    "        if isinstance(node, dict) and \"k1\" in node and \"k2\" in node:\n",
    "            return float(node[\"k1\"]), float(node[\"k2\"]), \"parent_safe\"\n",
    "        return float(global_fallback[\"k1\"]), float(global_fallback[\"k2\"]), \"global_fallback\"\n",
    "\n",
    "    if not isinstance(entry, dict):\n",
    "        if profile == \"safe\":\n",
    "            return fallback_safe()\n",
    "        return fallback_primary()\n",
    "\n",
    "    support = int(entry.get(\"support\", 0))\n",
    "    force_safe = bool((entry.get(\"guardrails\") or {}).get(\"force_safe\", False))\n",
    "    primary = entry.get(\"primary\") or {}\n",
    "    safe = entry.get(\"safe\") or {}\n",
    "\n",
    "    if not isinstance(primary, dict) or \"k1\" not in primary or \"k2\" not in primary:\n",
    "        p_k1, p_k2, _ = fallback_primary()\n",
    "    else:\n",
    "        p_k1, p_k2 = float(primary[\"k1\"]), float(primary[\"k2\"])\n",
    "\n",
    "    if not isinstance(safe, dict) or \"k1\" not in safe or \"k2\" not in safe:\n",
    "        s_k1, s_k2, _ = fallback_safe()\n",
    "    else:\n",
    "        s_k1, s_k2 = float(safe[\"k1\"]), float(safe[\"k2\"])\n",
    "\n",
    "    if profile == \"safe\":\n",
    "        return s_k1, s_k2, \"dim_safe\"\n",
    "    if profile == \"balanced\":\n",
    "        if force_safe:\n",
    "            return s_k1, s_k2, \"dim_safe_guardrail\"\n",
    "        return p_k1, p_k2, \"dim_primary\"\n",
    "    if force_safe or support < 100:\n",
    "        return s_k1, s_k2, \"dim_safe\"\n",
    "    return p_k1, p_k2, \"dim_primary\"\n",
    "\n",
    "\n",
    "def choose_model_type(table: dict, dim_key: str) -> str:\n",
    "    entry = (table.get(\"dimensions\") or {}).get(dim_key, {})\n",
    "    if isinstance(entry, dict):\n",
    "        model = entry.get(\"model\") or {}\n",
    "        if isinstance(model, dict):\n",
    "            kind = str(model.get(\"type\", \"\")).strip().lower()\n",
    "            if kind in {\"brown\", \"rational\"}:\n",
    "                return kind\n",
    "    defaults = table.get(\"model_defaults\") or {}\n",
    "    if isinstance(defaults, dict):\n",
    "        kind = str(defaults.get(\"type\", \"\")).strip().lower()\n",
    "        if kind in {\"brown\", \"rational\"}:\n",
    "            return kind\n",
    "    return \"brown\"\n",
    "\n",
    "\n",
    "def apply_base_undistortion(image: np.ndarray, k1: float, k2: float, model_type: str) -> np.ndarray:\n",
    "    h, w = image.shape[:2]\n",
    "    focal = float(np.sqrt(w * w + h * h))\n",
    "    camera = np.array([[focal, 0.0, w / 2.0], [0.0, focal, h / 2.0], [0.0, 0.0, 1.0]], dtype=np.float64)\n",
    "    if model_type == \"rational\":\n",
    "        dist = np.array([k1, k2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float64)\n",
    "    else:\n",
    "        dist = np.array([k1, k2, 0.0, 0.0, 0.0], dtype=np.float64)\n",
    "    new_camera, _ = cv2.getOptimalNewCameraMatrix(camera, dist, (w, h), 0.0, (w, h))\n",
    "    corrected = cv2.undistort(image, camera, dist, None, new_camera)\n",
    "    if corrected.shape[:2] != (h, w):\n",
    "        corrected = cv2.resize(corrected, (w, h), interpolation=cv2.INTER_LANCZOS4)\n",
    "    return corrected\n",
    "\n",
    "\n",
    "def compute_dense_flow(src_bgr: np.ndarray, tgt_bgr: np.ndarray) -> np.ndarray:\n",
    "    src = cv2.cvtColor(src_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    tgt = cv2.cvtColor(tgt_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(\n",
    "        src, tgt, None,\n",
    "        pyr_scale=0.5, levels=4, winsize=17,\n",
    "        iterations=4, poly_n=7, poly_sigma=1.4,\n",
    "        flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN,\n",
    "    )\n",
    "    return flow.astype(np.float32)\n",
    "\n",
    "\n",
    "def clip_flow_magnitude(flow: np.ndarray, clip_max: float) -> np.ndarray:\n",
    "    if clip_max <= 0:\n",
    "        return np.zeros_like(flow)\n",
    "    mag = np.sqrt(flow[..., 0] ** 2 + flow[..., 1] ** 2)\n",
    "    safe_mag = np.maximum(mag, 1e-6)\n",
    "    scale = np.minimum(1.0, clip_max / safe_mag)\n",
    "    return (flow * scale[..., None]).astype(np.float32)\n",
    "\n",
    "\n",
    "def aggregate_residual_flow(flow_stack: list[np.ndarray], flow_max_mag: float):\n",
    "    stack = np.stack(flow_stack, axis=0).astype(np.float32)\n",
    "    median_flow = np.median(stack, axis=0).astype(np.float32)\n",
    "    med_mag = np.sqrt(median_flow[..., 0] ** 2 + median_flow[..., 1] ** 2)\n",
    "    p99 = float(np.percentile(med_mag, 99.0)) if med_mag.size else 0.0\n",
    "    clip_max = float(min(max(p99, 0.0), max(flow_max_mag, 0.0)))\n",
    "    clipped = clip_flow_magnitude(median_flow, clip_max)\n",
    "    smoothed = cv2.GaussianBlur(clipped, (0, 0), sigmaX=FLOW_SMOOTH_SIGMA, sigmaY=FLOW_SMOOTH_SIGMA)\n",
    "    centered = stack - stack.mean(axis=0, keepdims=True)\n",
    "    var_scalar = float(np.mean(centered[..., 0] ** 2 + centered[..., 1] ** 2))\n",
    "    return smoothed.astype(np.float32), var_scalar, p99, clip_max\n",
    "\n",
    "\n",
    "def resize_flow_to_image(flow: np.ndarray, h: int, w: int) -> np.ndarray:\n",
    "    fh, fw = flow.shape[:2]\n",
    "    resized = cv2.resize(flow, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "    resized[..., 0] *= float(w) / float(fw)\n",
    "    resized[..., 1] *= float(h) / float(fh)\n",
    "    return resized.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_residual_flow(image: np.ndarray, flow: np.ndarray, lam: float) -> np.ndarray:\n",
    "    if lam <= 0.0:\n",
    "        return image.copy()\n",
    "    h, w = image.shape[:2]\n",
    "    f = resize_flow_to_image(flow, h, w) * float(lam)\n",
    "    ys, xs = np.mgrid[0:h, 0:w].astype(np.float32)\n",
    "    map_x = xs - f[..., 0]\n",
    "    map_y = ys - f[..., 1]\n",
    "    return cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "\n",
    "def compute_black_border_ratio(image: np.ndarray, threshold: int = 2) -> float:\n",
    "    return float(np.all(image <= threshold, axis=2).mean())\n",
    "\n",
    "\n",
    "def residual_warp_risk(flow: np.ndarray, lam: float) -> float:\n",
    "    if lam <= 0.0:\n",
    "        return 0.0\n",
    "    mag = np.sqrt(flow[..., 0] ** 2 + flow[..., 1] ** 2)\n",
    "    return float(abs(lam) * np.percentile(mag, 99.0)) if mag.size else 0.0\n",
    "\n",
    "\n",
    "def to_gray(image: np.ndarray) -> np.ndarray:\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if image.ndim == 3 else image\n",
    "\n",
    "\n",
    "def canny_union(gray: np.ndarray) -> np.ndarray:\n",
    "    e1 = cv2.Canny(gray, 50, 150)\n",
    "    e2 = cv2.Canny(gray, 80, 160)\n",
    "    e3 = cv2.Canny(gray, 110, 220)\n",
    "    return cv2.bitwise_or(cv2.bitwise_or(e1, e2), e3)\n",
    "\n",
    "\n",
    "def edge_similarity(pred: np.ndarray, gt: np.ndarray) -> float:\n",
    "    p = canny_union(to_gray(pred)) > 0\n",
    "    g = canny_union(to_gray(gt)) > 0\n",
    "    tp = int(np.logical_and(p, g).sum())\n",
    "    fp = int(np.logical_and(p, ~g).sum())\n",
    "    fn = int(np.logical_and(~p, g).sum())\n",
    "    if tp == 0 and fp == 0 and fn == 0:\n",
    "        return 1.0\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    return float((2.0 * tp) / (2.0 * tp + fp + fn))\n",
    "\n",
    "\n",
    "def line_hist(gray: np.ndarray, bins: int = 18) -> np.ndarray:\n",
    "    angles = []\n",
    "    if hasattr(cv2, \"createLineSegmentDetector\"):\n",
    "        lsd = cv2.createLineSegmentDetector()\n",
    "        lines = lsd.detect(gray)[0]\n",
    "        if lines is not None:\n",
    "            for item in lines:\n",
    "                x1, y1, x2, y2 = item[0]\n",
    "                angle = np.degrees(np.arctan2(float(y2 - y1), float(x2 - x1)))\n",
    "                angles.append((angle + 180.0) % 180.0)\n",
    "    if not angles:\n",
    "        edges = canny_union(gray)\n",
    "        min_line = max(8, min(gray.shape[0], gray.shape[1]) // 6)\n",
    "        lines = cv2.HoughLinesP(edges, 1, np.pi / 180.0, 30, minLineLength=min_line, maxLineGap=6)\n",
    "        if lines is not None:\n",
    "            for line in lines:\n",
    "                x1, y1, x2, y2 = line[0]\n",
    "                angle = np.degrees(np.arctan2(float(y2 - y1), float(x2 - x1)))\n",
    "                angles.append((angle + 180.0) % 180.0)\n",
    "    if not angles:\n",
    "        return np.zeros(bins, dtype=np.float32)\n",
    "    hist, _ = np.histogram(np.array(angles), bins=bins, range=(0.0, 180.0))\n",
    "    hist = hist.astype(np.float32)\n",
    "    total = float(hist.sum())\n",
    "    return hist / total if total > 0 else np.zeros(bins, dtype=np.float32)\n",
    "\n",
    "\n",
    "def line_orientation_loss(pred: np.ndarray, gt: np.ndarray) -> float:\n",
    "    p = line_hist(to_gray(pred))\n",
    "    g = line_hist(to_gray(gt))\n",
    "    ps, gs = float(p.sum()), float(g.sum())\n",
    "    if ps == 0.0 and gs == 0.0:\n",
    "        return 0.0\n",
    "    if ps == 0.0 or gs == 0.0:\n",
    "        return 1.0\n",
    "    return float(0.5 * np.abs(p - g).sum())\n",
    "\n",
    "\n",
    "def grad_hist(gray: np.ndarray, bins: int = 36) -> np.ndarray:\n",
    "    gx = cv2.Scharr(gray, cv2.CV_32F, 1, 0)\n",
    "    gy = cv2.Scharr(gray, cv2.CV_32F, 0, 1)\n",
    "    mag, ang = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "    mag = mag.astype(np.float32)\n",
    "    threshold = float(np.percentile(mag, 60.0)) if mag.size else 0.0\n",
    "    mask = mag > max(threshold, 1e-6)\n",
    "    if not np.any(mask):\n",
    "        return np.zeros(bins, dtype=np.float32)\n",
    "    hist, _ = np.histogram(ang[mask], bins=bins, range=(0.0, 360.0), weights=mag[mask])\n",
    "    hist = hist.astype(np.float32)\n",
    "    total = float(hist.sum())\n",
    "    return hist / total if total > 0 else np.zeros(bins, dtype=np.float32)\n",
    "\n",
    "\n",
    "def grad_orientation_loss(pred: np.ndarray, gt: np.ndarray) -> float:\n",
    "    p = grad_hist(to_gray(pred))\n",
    "    g = grad_hist(to_gray(gt))\n",
    "    pn, gn = float(np.linalg.norm(p)), float(np.linalg.norm(g))\n",
    "    if pn == 0.0 and gn == 0.0:\n",
    "        return 0.0\n",
    "    if pn == 0.0 or gn == 0.0:\n",
    "        return 1.0\n",
    "    cosine = float(np.dot(p, g) / (pn * gn + 1e-8))\n",
    "    return float(np.clip(1.0 - cosine, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def ssim_score(pred: np.ndarray, gt: np.ndarray) -> float:\n",
    "    x = to_gray(pred).astype(np.float32)\n",
    "    y = to_gray(gt).astype(np.float32)\n",
    "    if x.shape != y.shape:\n",
    "        y = cv2.resize(y, (x.shape[1], x.shape[0]))\n",
    "    mu_x = float(x.mean())\n",
    "    mu_y = float(y.mean())\n",
    "    sigma_x = float(((x - mu_x) ** 2).mean())\n",
    "    sigma_y = float(((y - mu_y) ** 2).mean())\n",
    "    sigma_xy = float(((x - mu_x) * (y - mu_y)).mean())\n",
    "    c1 = (0.01 * 255.0) ** 2\n",
    "    c2 = (0.03 * 255.0) ** 2\n",
    "    denom = (mu_x**2 + mu_y**2 + c1) * (sigma_x + sigma_y + c2)\n",
    "    if denom <= 0.0:\n",
    "        return 0.0\n",
    "    return float(np.clip(((2.0 * mu_x * mu_y + c1) * (2.0 * sigma_xy + c2)) / denom, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def normalized_mae(pred: np.ndarray, gt: np.ndarray) -> float:\n",
    "    if pred.shape != gt.shape:\n",
    "        gt = cv2.resize(gt, (pred.shape[1], pred.shape[0]))\n",
    "    return float(np.mean(np.abs(pred.astype(np.float32) - gt.astype(np.float32))) / 255.0)\n",
    "\n",
    "\n",
    "def competition_lite_loss(pred: np.ndarray, gt: np.ndarray) -> float:\n",
    "    edge = edge_similarity(pred, gt)\n",
    "    line = line_orientation_loss(pred, gt)\n",
    "    grad = grad_orientation_loss(pred, gt)\n",
    "    ssim = ssim_score(pred, gt)\n",
    "    mae = normalized_mae(pred, gt)\n",
    "    return float(\n",
    "        0.40 * (1.0 - edge)\n",
    "        + 0.22 * line\n",
    "        + 0.18 * grad\n",
    "        + 0.15 * (1.0 - ssim)\n",
    "        + 0.05 * mae\n",
    "    )\n",
    "\n",
    "\n",
    "def should_accept(proxy_gain: float, border_ratio: float, warp_risk: float, flow_var: float):\n",
    "    if flow_var > FLOW_STABILITY_VAR_MAX:\n",
    "        return False, \"reject_unstable_flow\"\n",
    "    if proxy_gain < ACCEPT_MIN_PROXY_GAIN:\n",
    "        return False, \"reject_low_proxy_gain\"\n",
    "    if border_ratio > BORDER_RATIO_MAX:\n",
    "        return False, \"reject_border_ratio\"\n",
    "    if warp_risk > WARP_RISK_MAX:\n",
    "        return False, \"reject_warp_risk\"\n",
    "    return True, \"accepted\"\n",
    "\n",
    "\n",
    "def parse_dim_key(dim_key: str):\n",
    "    w, h = dim_key.lower().split(\"x\", 1)\n",
    "    return int(w), int(h)\n",
    "\n",
    "\n",
    "def discover_data_dirs():\n",
    "    roots = [\n",
    "        Path(\"/kaggle/input/automatic-lens-correction\"),\n",
    "        Path(\"/kaggle/input/competitions/automatic-lens-correction\"),\n",
    "        Path(\"/kaggle/input\"),\n",
    "    ]\n",
    "    train_dir = None\n",
    "    test_dir = None\n",
    "\n",
    "    for root in roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for path in [root] + [p for p in root.rglob(\"*\") if p.is_dir()]:\n",
    "            if train_dir is None and any(path.glob(\"*_original.jpg\")):\n",
    "                train_dir = path\n",
    "            if test_dir is None:\n",
    "                jpg = list(path.glob(\"*.jpg\"))\n",
    "                if jpg and not any(path.glob(\"*_original.jpg\")) and not any(path.glob(\"*_generated.jpg\")):\n",
    "                    test_dir = path\n",
    "            if train_dir is not None and test_dir is not None:\n",
    "                break\n",
    "        if train_dir is not None and test_dir is not None:\n",
    "            break\n",
    "\n",
    "    if train_dir is None or test_dir is None:\n",
    "        raise RuntimeError(f\"Failed to discover data dirs. train={train_dir} test={test_dir}\")\n",
    "    return train_dir, test_dir\n",
    "\n",
    "\n",
    "def load_table():\n",
    "    env_path = os.getenv(\"DIM_TABLE_PATH\", \"\").strip()\n",
    "    candidates = []\n",
    "    if env_path:\n",
    "        candidates.append(Path(env_path))\n",
    "    candidates.extend([\n",
    "        Path(\"/kaggle/input/calibguard-dim-table/calibguard_dim_table.json\"),\n",
    "        Path(\"/kaggle/working/calibguard_dim_table.json\"),\n",
    "    ])\n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            return json.loads(path.read_text()), str(path)\n",
    "    return DEFAULT_DIM_TABLE, \"embedded_defaults\"\n",
    "\n",
    "\n",
    "def load_train_index(train_dir: Path):\n",
    "    pairs_by_dim = defaultdict(list)\n",
    "    originals = sorted([p for p in train_dir.glob(\"*_original.jpg\")])\n",
    "    for idx, orig in enumerate(originals, start=1):\n",
    "        gen = train_dir / orig.name.replace(\"_original.jpg\", \"_generated.jpg\")\n",
    "        if not gen.exists():\n",
    "            continue\n",
    "        img = cv2.imread(str(orig))\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        pairs_by_dim[f\"{w}x{h}\"].append((orig, gen))\n",
    "        if idx % 4000 == 0:\n",
    "            print(f\"Indexed {idx}/{len(originals)} train originals\")\n",
    "    return pairs_by_dim\n",
    "\n",
    "\n",
    "def evaluate_lambdas(holdout_pairs, k1, k2, model_type, flow_map):\n",
    "    stats = {}\n",
    "    for lam in sorted(set([0.0] + LAMBDA_GRID)):\n",
    "        losses = []\n",
    "        borders = []\n",
    "        for orig, target in holdout_pairs:\n",
    "            base = apply_base_undistortion(orig, k1, k2, model_type)\n",
    "            pred = apply_residual_flow(base, flow_map, lam)\n",
    "            losses.append(competition_lite_loss(pred, target))\n",
    "            borders.append(compute_black_border_ratio(pred))\n",
    "        if losses:\n",
    "            stats[lam] = (float(np.mean(losses)), float(np.mean(borders)))\n",
    "        else:\n",
    "            stats[lam] = (float(\"inf\"), 1.0)\n",
    "\n",
    "    base_loss = stats[0.0][0]\n",
    "    best_lam = min(stats, key=lambda x: stats[x][0])\n",
    "    best_loss, best_border = stats[best_lam]\n",
    "    return best_lam, base_loss, best_loss, best_border\n",
    "\n",
    "\n",
    "def calibrate_dim_model(dim_key, pairs, table):\n",
    "    w, h = parse_dim_key(dim_key)\n",
    "    parent = classify_parent(h, w)\n",
    "    k1, k2, coeff_source = choose_coeffs(table, dim_key, parent, PROFILE)\n",
    "    model_type = choose_model_type(table, dim_key)\n",
    "\n",
    "    sample = list(pairs)\n",
    "    random.shuffle(sample)\n",
    "    sample = sample[: min(len(sample), TRAIN_SAMPLE_PER_DIM)]\n",
    "\n",
    "    if len(sample) < 4:\n",
    "        return DimModel(dim_key, len(pairs), parent, k1, k2, model_type, 0.0, False, \"reject_not_enough_samples\", 0.0, float(\"inf\"), float(\"inf\"), 1.0, float(\"inf\"), 0.0, 0.0, 0.0, None)\n",
    "\n",
    "    split_idx = max(1, int(round(len(sample) * (1.0 - HOLDOUT_RATIO))))\n",
    "    split_idx = min(split_idx, len(sample) - 1)\n",
    "    train_subset = sample[:split_idx]\n",
    "    holdout_subset = sample[split_idx:]\n",
    "\n",
    "    flow_stack = []\n",
    "    for orig_path, gen_path in train_subset:\n",
    "        orig = cv2.imread(str(orig_path))\n",
    "        target = cv2.imread(str(gen_path))\n",
    "        if orig is None or target is None:\n",
    "            continue\n",
    "        base = apply_base_undistortion(orig, k1, k2, model_type)\n",
    "        if FLOW_WORK_SCALE != 1.0:\n",
    "            base = cv2.resize(base, None, fx=FLOW_WORK_SCALE, fy=FLOW_WORK_SCALE, interpolation=cv2.INTER_AREA)\n",
    "            target = cv2.resize(target, (base.shape[1], base.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "        flow_stack.append(compute_dense_flow(base, target))\n",
    "\n",
    "    if not flow_stack:\n",
    "        return DimModel(dim_key, len(pairs), parent, k1, k2, model_type, 0.0, False, \"reject_no_flow_pairs\", 0.0, float(\"inf\"), float(\"inf\"), 1.0, float(\"inf\"), 0.0, 0.0, 0.0, None)\n",
    "\n",
    "    flow_map, flow_var, flow_p99, flow_clip_max = aggregate_residual_flow(flow_stack, FLOW_MAX_MAG)\n",
    "\n",
    "    holdout_pairs = []\n",
    "    for orig_path, gen_path in holdout_subset:\n",
    "        orig = cv2.imread(str(orig_path))\n",
    "        target = cv2.imread(str(gen_path))\n",
    "        if orig is None or target is None:\n",
    "            continue\n",
    "        holdout_pairs.append((orig, target))\n",
    "\n",
    "    best_lam, base_loss, best_loss, best_border = evaluate_lambdas(holdout_pairs, k1, k2, model_type, flow_map)\n",
    "    proxy_gain = float(base_loss - best_loss)\n",
    "    warp = residual_warp_risk(flow_map, best_lam)\n",
    "    accepted, reason = should_accept(proxy_gain, best_border, warp, flow_var)\n",
    "    if not accepted:\n",
    "        best_lam = 0.0\n",
    "\n",
    "    return DimModel(\n",
    "        dim_key=dim_key,\n",
    "        support=len(pairs),\n",
    "        parent_class=parent,\n",
    "        k1=k1,\n",
    "        k2=k2,\n",
    "        model_type=model_type,\n",
    "        lambda_value=float(best_lam),\n",
    "        accepted=accepted,\n",
    "        reason=reason,\n",
    "        proxy_gain=float(proxy_gain),\n",
    "        base_loss=float(base_loss),\n",
    "        best_loss=float(best_loss),\n",
    "        border_ratio=float(best_border),\n",
    "        flow_var=float(flow_var),\n",
    "        flow_p99=float(flow_p99),\n",
    "        flow_clip_max=float(flow_clip_max),\n",
    "        warp_risk=float(warp),\n",
    "        flow_map=flow_map if accepted else None,\n",
    "    )\n",
    "\n",
    "\n",
    "def zip_dir(input_dir: Path, zip_path: Path, names: list[str]):\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_STORED) as zf:\n",
    "        for name in names:\n",
    "            path = input_dir / name\n",
    "            if path.exists():\n",
    "                zf.write(path, name)\n",
    "\n",
    "\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    train_dir, test_dir = discover_data_dirs()\n",
    "    table, table_source = load_table()\n",
    "\n",
    "    print(f\"Train dir: {train_dir}\")\n",
    "    print(f\"Test dir: {test_dir}\")\n",
    "    print(f\"Table source: {table_source}\")\n",
    "\n",
    "    train_index = load_train_index(train_dir)\n",
    "\n",
    "    dim_entries = table.get(\"dimensions\", {}) if isinstance(table.get(\"dimensions\"), dict) else {}\n",
    "    if not dim_entries:\n",
    "        # if table has no exact dims, infer from observed train dims using parent defaults\n",
    "        for dim_key, pairs in train_index.items():\n",
    "            dim_entries[dim_key] = {\"support\": len(pairs)}\n",
    "\n",
    "    eligible_dims = []\n",
    "    for dim_key, entry in dim_entries.items():\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        support = int(entry.get(\"support\", 0))\n",
    "        if support < MIN_SUPPORT:\n",
    "            continue\n",
    "        if dim_key not in train_index:\n",
    "            continue\n",
    "        eligible_dims.append(dim_key)\n",
    "    eligible_dims = sorted(eligible_dims)\n",
    "\n",
    "    print(f\"Eligible dimensions: {len(eligible_dims)}\")\n",
    "\n",
    "    models = {}\n",
    "    for idx, dim_key in enumerate(eligible_dims, start=1):\n",
    "        model = calibrate_dim_model(dim_key, train_index[dim_key], table)\n",
    "        models[dim_key] = model\n",
    "        print(\n",
    "            f\"[{idx}/{len(eligible_dims)}] {dim_key} support={model.support} \"\n",
    "            f\"lambda={model.lambda_value:.3f} gain={model.proxy_gain:.5f} \"\n",
    "            f\"accepted={model.accepted} ({model.reason})\"\n",
    "        )\n",
    "\n",
    "    test_files = sorted([p.name for p in test_dir.glob(\"*.jpg\")])\n",
    "\n",
    "    source_counts = Counter()\n",
    "    parent_counts = Counter()\n",
    "\n",
    "    for idx, name in enumerate(test_files, start=1):\n",
    "        image = cv2.imread(str(test_dir / name))\n",
    "        if image is None:\n",
    "            continue\n",
    "        h, w = image.shape[:2]\n",
    "        dim_key = f\"{w}x{h}\"\n",
    "        parent = classify_parent(h, w)\n",
    "        parent_counts[parent] += 1\n",
    "\n",
    "        k1, k2, _source = choose_coeffs(table, dim_key, parent, PROFILE)\n",
    "        model_type = choose_model_type(table, dim_key)\n",
    "        base = apply_base_undistortion(image, k1, k2, model_type)\n",
    "\n",
    "        cv2.imwrite(str(BASE_DIR / name), base, [cv2.IMWRITE_JPEG_QUALITY, JPEG_QUALITY])\n",
    "\n",
    "        dm = models.get(dim_key)\n",
    "        if dm is not None and dm.accepted and dm.flow_map is not None:\n",
    "            pred = apply_residual_flow(base, dm.flow_map, dm.lambda_value)\n",
    "            source_counts[\"base_plus_residual\"] += 1\n",
    "        else:\n",
    "            pred = base\n",
    "            source_counts[\"base_only\"] += 1\n",
    "\n",
    "        cv2.imwrite(str(CANDIDATE_DIR / name), pred, [cv2.IMWRITE_JPEG_QUALITY, JPEG_QUALITY])\n",
    "\n",
    "        if idx % 100 == 0 or idx == len(test_files):\n",
    "            print(f\"Rendered {idx}/{len(test_files)}\")\n",
    "\n",
    "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    candidate_zip = WORK_ROOT / f\"submission_flow_residual_dim_v1_{ts}.zip\"\n",
    "    base_zip = WORK_ROOT / f\"submission_flow_residual_dim_base_{ts}.zip\"\n",
    "    summary_json = WORK_ROOT / f\"run_summary_{ts}.json\"\n",
    "\n",
    "    zip_dir(CANDIDATE_DIR, candidate_zip, test_files)\n",
    "    zip_dir(BASE_DIR, base_zip, test_files)\n",
    "\n",
    "    dim_summary = {}\n",
    "    for dim_key, model in models.items():\n",
    "        dim_summary[dim_key] = {\n",
    "            \"support\": model.support,\n",
    "            \"parent_class\": model.parent_class,\n",
    "            \"base_coeffs\": {\"k1\": model.k1, \"k2\": model.k2, \"model_type\": model.model_type},\n",
    "            \"lambda\": model.lambda_value,\n",
    "            \"proxy_gain\": model.proxy_gain,\n",
    "            \"base_loss\": model.base_loss,\n",
    "            \"best_loss\": model.best_loss,\n",
    "            \"border_ratio\": model.border_ratio,\n",
    "            \"flow_var\": model.flow_var,\n",
    "            \"flow_p99\": model.flow_p99,\n",
    "            \"flow_clip_max\": model.flow_clip_max,\n",
    "            \"warp_risk\": model.warp_risk,\n",
    "            \"accepted\": model.accepted,\n",
    "            \"reason\": model.reason,\n",
    "        }\n",
    "\n",
    "    summary = {\n",
    "        \"method\": \"flow_residual_dim_v1\",\n",
    "        \"timestamp_utc\": datetime.utcnow().isoformat(),\n",
    "        \"train_dir\": str(train_dir),\n",
    "        \"test_dir\": str(test_dir),\n",
    "        \"table_source\": table_source,\n",
    "        \"config\": {\n",
    "            \"min_support\": MIN_SUPPORT,\n",
    "            \"train_sample_per_dim\": TRAIN_SAMPLE_PER_DIM,\n",
    "            \"holdout_ratio\": HOLDOUT_RATIO,\n",
    "            \"flow_max_mag\": FLOW_MAX_MAG,\n",
    "            \"lambda_grid\": sorted(set([0.0] + LAMBDA_GRID)),\n",
    "            \"accept_min_proxy_gain\": ACCEPT_MIN_PROXY_GAIN,\n",
    "            \"profile\": PROFILE,\n",
    "            \"alpha_forced\": 0.0,\n",
    "        },\n",
    "        \"thresholds\": {\n",
    "            \"border_ratio_max\": BORDER_RATIO_MAX,\n",
    "            \"warp_risk_max\": WARP_RISK_MAX,\n",
    "            \"flow_stability_var_max\": FLOW_STABILITY_VAR_MAX,\n",
    "        },\n",
    "        \"counts\": {\n",
    "            \"test_images\": len(test_files),\n",
    "            \"eligible_dimensions\": len(eligible_dims),\n",
    "            \"accepted_dimensions\": int(sum(1 for m in models.values() if m.accepted)),\n",
    "            \"source_counts\": dict(source_counts),\n",
    "            \"parent_counts\": dict(parent_counts),\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"candidate_zip\": str(candidate_zip),\n",
    "            \"base_zip\": str(base_zip),\n",
    "            \"recommended_submission_zip\": str(candidate_zip),\n",
    "        },\n",
    "        \"dimensions\": dim_summary,\n",
    "        \"elapsed_seconds\": float(time.time() - t0),\n",
    "    }\n",
    "\n",
    "    summary_json.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n=== COMPLETE ===\")\n",
    "    print(f\"Candidate zip: {candidate_zip}\")\n",
    "    print(f\"Base zip: {base_zip}\")\n",
    "    print(f\"Run summary: {summary_json}\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}